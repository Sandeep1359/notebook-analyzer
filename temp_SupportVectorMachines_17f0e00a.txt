[MARKDOWN CELL]


[MARKDOWN CELL]
# Introduction - SVM

[MARKDOWN CELL]
SVM is the Supervised Machine Learning algorithm used for both classification, regression. But mostly preferred for classification.

Given a dataset, the algorithm tries to divide the data using hyperplanes and then makes the predictions. SVM is a non-probabilistic linear classifier. While other classifiers, when classifying, predict the probability of a data point to belong to one group or the another, SVM directly says to which group the datapoint belongs to without using any probability calculation.


[MARKDOWN CELL]

How it works?
- SVM constructs a best line or the decision boundary called **Hyperplane** which can be used for classification or regression or outlier detection.  The dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane.

- This hyperplane creates 2 margin lines parallel to it which have some distance so that it can distinctly classify the data points. The distance between the 2 margin lines are called **marginal distance**.

- These 2 margin lines passes through the most nearest +ve points and the most nearest -ve points. Those points through which the margin lines pass are called **support vectors**. Support vectors are important as it helps to determine the maximum distance of the marginal plane.


[MARKDOWN CELL]
## Understanding the Mathematics involved
Let’s take the example of the following dataset and see how can we divide the data into appropriate groups.
<img src='SVM_intution.PNG'  width="300">

[MARKDOWN CELL]
We can see that there are two groups of data. The question is how to divide these points into two groups. It can be done using any of the three lines. Or, for that purpose, there can be an infinite number of straight lines that can divide these points into two classes. Now, which line to choose?
SVM solves this problem using the maximum margin as shown
<img src='SVM_hyperplane.PNG' width="400">


[MARKDOWN CELL]
The black line in the middle is the optimum classifier. This line is drawn to maximise the distance of the classifier line from the nearest points in the two classes. It is also called a __hyperplane__ in terms of  SVM.
A _Hyperplane_ is an n-1 dimensional plane which optimally divides the data of n dimensions. Here, as we have only a 2-D data, so the hyperplane can be represented using one dimension only. Hence, the hyperplane is a line here.
The two points (highlighted with circles) which are on the yellow lines, they are called the __support vectors__. As it is a 2-D figure, they are points. In a multi-dimensional space, they will be vectors, and hence, the name- support vector machine as the algorithm creates the optimum classification line by maximising its distance from the two support vectors.

When the data is not linearly separable,  then to create a hyperplane to separate data into different groups, the SVM algorithm needs to perform computations in a higher-dimensional space. But the introduction of new dimensions makes the computations for the SVMs more intensive, which impacts the algorithm performance. To rectify this, mathematicians came up with the approach of Kernel methods.
Kernel methods use kernel functions available in mathematics. The unique feature of a kernel function is to compute in a higher-dimensional space without calculating the new coordinates in that higher dimension. It implicitly uses predefined mathematical functions to do operations on the existing points which mimic the computation in a higher-dimensional space without adding to the computation cost as they are not actually calculating the coordinates in the higher dimension thereby avoiding the computation of calculating distances from the newly computed points.  This is called the kernel trick.
<img src= "SVM_3D_Hyperplane.PNG" width="300">
                                                                        Image: bogotobogo.com


[MARKDOWN CELL]
In the left diagram above, we have a non-linear distribution of data as we can not classify a data using a linear equation. To solve this problem, we can project the points in a 3-dimensional space and then derive a plane which divides the data into two parts. In theory, that’s what a kernel function does without computing the additional coordinates for the higher dimension.


[MARKDOWN CELL]
## Support Vector Regression

[MARKDOWN CELL]
Let’s talk about Linear Regression first. How to determine the best fit line? The idea is to create a line which minimises the total residual error. The SVR approach is a bit different. Instead of trying to minimise the error, SVR focuses on keeping the error in a fixed range. This approach can be explained using three lines. The first line is the best fit regressor line, and the other two lines are the bordering ones which denote the range of error.
<img src="SVR.PNG" width="500">

[MARKDOWN CELL]
What does this mean? It means that we are going to consider the points inside this ± error boundary only for preparing our model. In other words, the best fit line(or the hyperplane) will be the line which goes through the maximum number of data points and the error boundaries are chosen to ensure maximum inclusion. This error term can be customized  using the '_epsilon_' parameter defined for the scikit-learn SVR implementation.  

[CODE CELL]


[MARKDOWN CELL]
## Python Implementation

[MARKDOWN CELL]
### Business Case:-To find out based on given features whether the loan will get approved or not

[CODE CELL]
## Supervised learning with classification task(2 classes)

[CODE CELL]
##importing the libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

[CODE CELL]
## loading the data
data=pd.read_csv('loan_approved.csv')

[CODE CELL]
## Getting the first few rows of data
data.head()

[CODE CELL]
data.Loan_Amount_Term.value_counts()

[CODE CELL]
# Domian analysis.

[MARKDOWN CELL]
## Basic Checks

[CODE CELL]
head,tail,info,describe for both numerical and categorical

[CODE CELL]
data.describe(include='O')

[CODE CELL]
data.info()

[MARKDOWN CELL]
## Exploratory Data Analysis

[CODE CELL]
# Univariate analysis
# Bivaraite analysis--Autoviz

[CODE CELL]
!pip install sweetviz

[CODE CELL]
import sweetviz as sv #  library for univariant analysis

my_report = sv.analyze(data)## pass the original dataframe

my_report.show_html() # Default arguments will generate to "SWEETVIZ_REPORT.html"

[CODE CELL]
## Bivariate Analysis
data.info()

[CODE CELL]
data1=data[['Gender','Married','Dependents','Education','Self_Employed','Property_Area']]
data2=data[['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']]

[CODE CELL]
plt.figure(figsize=(20,25), facecolor='white')#To set canvas
plotnumber = 1#counter

for column in data1:#accessing the columns
    if plotnumber<=16 :
        ax = plt.subplot(4,4,plotnumber)
        sns.countplot(x=data1[column],hue=data['Loan_Status (Approved)'])
        plt.xlabel(column,fontsize=20)#assign name to x-axis and set font-20
        plt.ylabel('Loan Status',fontsize=20)
    plotnumber+=1#counter increment
plt.tight_layout()

[CODE CELL]
plt.figure(figsize=(20,25), facecolor='white')#To set canvas
plotnumber = 1#counter

for column in data2:#accessing the columns
    if plotnumber<=16 :
        ax = plt.subplot(4,4,plotnumber)
        sns.histplot(x=data2[column],hue=data['Loan_Status (Approved)'])
        plt.xlabel(column,fontsize=20)#assign name to x-axis and set font-20
        plt.ylabel('Loan Status',fontsize=20)
    plotnumber+=1#counter increment
plt.tight_layout()

[MARKDOWN CELL]
## Data Preprocessing Pipeline

[CODE CELL]
## Checking missing values

data.isnull().sum()

[CODE CELL]
data.shape

[CODE CELL]
## Getting the rows where values are missed in Gender features
data.loc[data['Gender'].isnull()==True]

[CODE CELL]
## Checking the distribution along the both labels
data.Gender.value_counts()

[CODE CELL]
## How much values have been missed
data.Gender.isnull().sum()/len(data)*100

[CODE CELL]
data.head()

[CODE CELL]
import seaborn as sns

sns.countplot(x='Gender',hue='Loan_Status (Approved)',data=data)

[CODE CELL]
## Imputing the missing values with mode
data.loc[data['Gender'].isnull()==True,'Gender']='Male'

[CODE CELL]
data.Gender.isnull().sum()

[CODE CELL]
## Getting the values in Dependents
data.loc[data['Dependents'].isnull()==True]

[CODE CELL]
data.Dependents.value_counts()

[CODE CELL]
sns.countplot(x='Dependents',data=data,hue='Loan_Status')

[CODE CELL]
pd.crosstab(data.Dependents,data.Loan_Status)

[CODE CELL]
## renaming the target column
data.rename(columns={"Loan_Status (Approved)":'Loan_Status'},inplace=True)

[CODE CELL]
# From the graphical representation it can be seen that as the number of dependents
#getting increased,
# the chances of approval is less, since we have missed values and if we approve
# loan for them
# it can turn into major loss(high chance).Hence we will substitue missing values
#by 3+.
data.loc[data['Dependents'].isnull()==True,'Dependents']='3+'

[CODE CELL]
data

[CODE CELL]
## For married feature
data.loc[data['Married'].isnull()==True]

[CODE CELL]
sns.countplot(x='Married',hue='Loan_Status',data=data)

[CODE CELL]
## getting the counts
data.Married.value_counts()

[CODE CELL]
## Imputing with yes i.e mode
data.loc[data['Married'].isnull()==True,'Married']='Yes'

[CODE CELL]
## Checking Self_Employed
#data.loc[data['Self_Employed'].isnull()==True]
data.loc[data['Self_Employed']=='No']

[CODE CELL]
sns.countplot(x='Self_Employed',hue='Loan_Status',data=data)

[CODE CELL]
pd.crosstab(data.Self_Employed,data.Loan_Status)

[CODE CELL]
data.Self_Employed.value_counts()

[CODE CELL]
sns.countplot(x='Self_Employed',data=data,hue='Loan_Status')

[CODE CELL]
# Replace the nan values with mode
data.loc[data['Self_Employed'].isnull()==True,'Self_Employed']='No'

[CODE CELL]
# check for null values
data.isnull().sum()

[CODE CELL]
## Histogram since it has numerical value
data.LoanAmount.hist()
plt.show()

[MARKDOWN CELL]
Since data is skewed, we can use median to replace the nan value. It is recommended to use mean only for symmetric data distribution.

[CODE CELL]
# Check median for LoanAmount column
np.median(data.LoanAmount.dropna(axis=0))

[CODE CELL]
# Replace the nan values in LoanAmount column with median value
data.loc[data['LoanAmount'].isnull()==True,'LoanAmount']=np.median(data.LoanAmount.dropna(axis=0))

[CODE CELL]
data.LoanAmount.isnull().sum()

[CODE CELL]
## Laon amount term

[CODE CELL]
data.Loan_Amount_Term.isnull().sum()

[CODE CELL]
data.Loan_Amount_Term.value_counts()

[CODE CELL]
data.Loan_Amount_Term.median()

[CODE CELL]
data.Loan_Amount_Term.hist()

[CODE CELL]
# replace the nan values in Loan_Amount_Term with the median value
data.loc[data['Loan_Amount_Term'].isnull()==True,'Loan_Amount_Term']=np.median(data.Loan_Amount_Term.dropna(axis=0))

[CODE CELL]
# Credit_History
data.Credit_History.value_counts()

[CODE CELL]
sns.countplot(x='Credit_History',data=data,hue='Loan_Status')

[CODE CELL]
data.loc[data['Credit_History'].isnull()==True,'Credit_History']=0.0

[CODE CELL]
data.isnull().sum()

[CODE CELL]
## Step 2 Handling the categorical data
data.info()

[CODE CELL]
data.Education.value_counts()

[CODE CELL]
## Using label encoder to convert the categorical data to numerical data
## Donot run this code.This is just implementation of label encoder.This dataset have lots relationship with target.
from sklearn.preprocessing import LabelEncoder
lc=LabelEncoder()

data.Gender=lc.fit_transform(data.Gender)
data.Married=lc.fit_transform(data.Married)
data.Education=lc.fit_transform(data.Education)
data.Property_Area=lc.fit_transform(data.Property_Area)
data.Loan_Status=lc.fit_transform(data.Loan_Status)
data.Dependents=lc.fit_transform(data.Dependents)
data.Self_Employed=lc.fit_transform(data.Self_Employed)

[CODE CELL]
data.Education.unique()

[CODE CELL]
data.head()

[CODE CELL]
data.head()

[CODE CELL]
pd.get_dummies(data['Gender'],prefix='Gender')

[CODE CELL]
pd.get_dummies(data['Gender'],prefix='Gender',drop_first=True)

[CODE CELL]
## One hot encoding
df1=pd.get_dummies(data['Gender'],prefix='Gender',drop_first=True)
data=pd.concat([data,df1],axis=1).drop(['Gender'],axis=1)

[CODE CELL]
data.head()

[CODE CELL]
df1=pd.get_dummies(data['Married'],prefix='Married',drop_first=True)
data=pd.concat([data,df1],axis=1).drop(['Married'],axis=1)

[CODE CELL]
df1=pd.get_dummies(data['Education'],prefix='Education',drop_first=True)
data=pd.concat([data,df1],axis=1).drop(['Education'],axis=1)

[CODE CELL]
df1=pd.get_dummies(data['Property_Area'],prefix='Property_Area',drop_first=True)
data=pd.concat([data,df1],axis=1).drop(['Property_Area'],axis=1)

[CODE CELL]
df1=pd.get_dummies(data['Dependents'],prefix='Dependents',drop_first=True)
data=pd.concat([data,df1],axis=1).drop(['Dependents'],axis=1)

[CODE CELL]
df1=pd.get_dummies(data['Self_Employed'],prefix='Self_Employed',drop_first=True)
data=pd.concat([data,df1],axis=1).drop(['Self_Employed'],axis=1)

[CODE CELL]
data.head()

[CODE CELL]
## scaling data
from sklearn.preprocessing import MinMaxScaler
scale=MinMaxScaler()
data[['ApplicantIncome','CoapplicantIncome','LoanAmount']]=scale.fit_transform(data[['ApplicantIncome','CoapplicantIncome',
                          'LoanAmount']])



[CODE CELL]
data.describe()

[CODE CELL]
## checking the duplicate rows

#data.duplicate()
data.duplicated().sum()

[CODE CELL]
## Saving the preprocessed data.
data.to_csv('Preprocessed_data.csv')

[CODE CELL]
## Loading the data
preprcessed_data=pd.read_csv('Preprocessed_data.csv')

[CODE CELL]
preprcessed_data

[MARKDOWN CELL]
## Feature Selection

[CODE CELL]
# Removing redundant columns
#We can drop loan id.
l1=['Unnamed: 0','Loan_ID']
preprcessed_data.drop(l1,axis=1,inplace=True)

[CODE CELL]
preprcessed_data

[CODE CELL]
## checking correlation
corr_data=preprcessed_data[['ApplicantIncome','CoapplicantIncome','LoanAmount']]

[CODE CELL]
sns.heatmap(corr_data.corr(),annot=True)

[CODE CELL]
## There is no relationship among the numerical data

[CODE CELL]
corr_data.describe() ## no constant features

[CODE CELL]
data.head()

[CODE CELL]
pre_data.Loan_Status=data.Loan_Status.map({'Y':1,'N':0})

[MARKDOWN CELL]
## Model Creation

[CODE CELL]
preprcessed_data.head(1)

[CODE CELL]
preprcessed_data.columns

[CODE CELL]
preprcessed_data.Loan_Status=preprcessed_data.Loan_Status.map({'Y':1,'N':0})

[CODE CELL]
## defining X and y
X=preprcessed_data.loc[:,['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
       'Loan_Amount_Term', 'Credit_History','Gender_Male',
       'Married_Yes', 'Education_Not Graduate', 'Property_Area_Semiurban',
       'Property_Area_Urban', 'Dependents_1', 'Dependents_2', 'Dependents_3+',
       'Self_Employed_Yes']]
y=preprcessed_data.Loan_Status

[CODE CELL]
X

[CODE CELL]
y

[CODE CELL]
## creating training and testing data
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X, y,random_state=3)

[CODE CELL]
## balancing the data
data.Loan_Status.value_counts()

[CODE CELL]
# Install imblearn package - pip install imblearn
from imblearn.over_sampling import SMOTE
smote = SMOTE()

[CODE CELL]
X_smote, y_smote = smote.fit_resample(X_train,y_train)

[MARKDOWN CELL]
**Counter** is a container which keeps track to how many times equivalent values are added. Python counter class is a part of collections module and is a subclass of dictionary.

[CODE CELL]
from collections import Counter
print("Actual Classes",Counter(y_train))
print("SMOTE Classes",Counter(y_smote))

[CODE CELL]


[CODE CELL]
# Support Vector Classifier Model

from sklearn.svm import SVC
svclassifier = SVC() ## base model with default parameters
svclassifier.fit(X_smote, y_smote)

[CODE CELL]
# Predict output for X_test

y_hat=svclassifier.predict(X_test)

[CODE CELL]
## evaluating the model created
from sklearn.metrics import accuracy_score,recall_score,precision_score,classification_report,f1_score
acc=accuracy_score(y_test,y_hat)
acc

[CODE CELL]
# Calssification report measures the quality of predictions. True Positives, False Positives, True negatives and False Negatives
# are used to predict the metrics of a classification report

print(classification_report(y_test,y_hat))

[CODE CELL]
cm1=pd.crosstab(y_test,y_hat)
cm1

[CODE CELL]
# F1 score considers both Precision and Recall for evaluating a model
f1=f1_score(y_test,y_hat)
f1

[CODE CELL]
## checking cross validation score
from sklearn.model_selection import cross_val_score
scores = cross_val_score(svclassifier,X,y,cv=3,scoring='f1')
print(scores)
print("Cross validation Score:",scores.mean())
print("Std :",scores.std())
#std of < 0.05 is good.

[MARKDOWN CELL]
## What is a Model Hyperparameter?

A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.



[MARKDOWN CELL]
## Hyperparameters of Support Vector Machine

[MARKDOWN CELL]
#### SVM separates data points that belong to different classes with a decision boundary. When determining the decision boundary, a soft margin SVM (soft margin means allowing some data points to be misclassified) tries to solve an optimization problem with the following goals:

#### 1)Increase the distance of decision boundary to classes (or support vectors)
#### 2)Maximize the number of points that are correctly classified in the training set

[MARKDOWN CELL]
### There is obviously a trade-off between these two goals which and it is controlled by C which adds a penalty for each misclassified data point.

### If C is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassification.

### If C is large, SVM tries to minimize the number of misclassified examples due to the high penalty which results in a decision boundary with a smaller margin. The penalty is not the same for all misclassified examples. It is directly proportional to the distance to the decision boundary.

[MARKDOWN CELL]
<img src='1_XFtyzSNjexMecQ4wmqBfgA.PNG'  width="300">

[MARKDOWN CELL]
<img src='1_k4wh7vzjDbQWXx7wKyH0kg.PNG'  width="600">



[MARKDOWN CELL]
### Gamma is a hyperparameter used with non-linear SVM. One of the most commonly used non-linear kernels is the radial basis function (RBF). Gamma parameter of RBF controls the distance of the influence of a single training point.

### Low values of gamma indicate a large similarity radius which results in more points being grouped together.

### For high values of gamma, the points need to be very close to each other in order to be considered in the same group (or class). Therefore, models with very large gamma values tend to overfit.

[MARKDOWN CELL]
<img src='1_JDSwT-svWnAu69fy9oguBw.png' width="600">

[MARKDOWN CELL]

<img src='1_faj7x1I0uFwfU6mkLfUwvg.png' width="600">

[MARKDOWN CELL]
<img src='1_5DtPKUzLI1e-FIjC-odFiw.png' width="600">

[MARKDOWN CELL]
## GridSearchCV

[MARKDOWN CELL]
#### It is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. As mentioned above, the performance of a model significantly depends on the value of hyperparameters

#### Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.

[MARKDOWN CELL]
#### GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance.

[CODE CELL]
from sklearn.model_selection import GridSearchCV

# defining parameter range
param_grid = {'C': [0.1, 5, 10,50,60,70],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
             'random_state':(list(range(1, 20)))}
model=SVC()
grid = GridSearchCV(model, param_grid, refit = True, verbose = 2,scoring='f1',cv=5)

# fitting the model for grid search
grid.fit(X,y)

[CODE CELL]
# print best parameter after tuning
print(grid.best_params_)

# print how our model looks after hyper-parameter tuning
#print(grid.best_estimator_)

[CODE CELL]
#clf=SVC(C=100, gamma=0.001,random_state=42) ##0.1
clf=SVC(C=5, gamma=0.1,random_state=1) ##0.1

[CODE CELL]
clf.fit(X_smote, y_smote)

[CODE CELL]
y_clf=clf.predict(X_test)

[CODE CELL]
print(classification_report(y_test,y_clf))

[CODE CELL]
cm=pd.crosstab(y_test,y_clf)
cm

[CODE CELL]
f1=f1_score(y_test,y_clf)
f1

[CODE CELL]
scores_after = cross_val_score(clf,X,y,cv=3,scoring='f1')
print(scores_after)
print("Cross validation Score:",scores_after.mean())
print("Std :",scores.std())
#std of < 0.05 is good.

[MARKDOWN CELL]
# If you can see f1 score is improved however the recall is still lagging.

# Task:-Change the all preprocessing technqiue and try to see if performance can be improved for not.

# We still have ot gone through feature selection technqiue.


[CODE CELL]


[MARKDOWN CELL]
   # PROJECT ID :  "PRCP-1020-House Price Prediction"
# Project Team ID : " PTID-CDS-FEB-25-2476"  
            


[MARKDOWN CELL]
**1. Load Data**

[MARKDOWN CELL]
First lets import all the libraries that will be used to load train and test datasets and data manipulation.

[CODE CELL]
# Import libraries

# Pandas
import pandas as pd
from pandas import Series,DataFrame

# Numpy and Matplotlib
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#sns.set_style('whitegrid')
%matplotlib inline

# Machine Learning
from sklearn import preprocessing

[MARKDOWN CELL]
Loading train and test data

[CODE CELL]
# Get Data in Dataframe
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

[MARKDOWN CELL]
Preview of train and test data

[CODE CELL]
# head() shows the first 5 rows of the data
train.head()

[CODE CELL]
test.head()

[MARKDOWN CELL]
There are 1460 entries in the train data set and 1459 entries in test data set. The data contains some NaN values too.

[CODE CELL]
train.info()

[CODE CELL]
test.info()

[MARKDOWN CELL]
**2. Data Manipulation and Visualization**

[MARKDOWN CELL]
Lets check for NaN (null) values in the data

[CODE CELL]
train.isnull().sum()

[CODE CELL]
test.isnull().sum()

[MARKDOWN CELL]
Lets check for the mean, standard deviation for Sales price

[CODE CELL]
train['SalePrice'].describe()

[MARKDOWN CELL]
Sales price is right skewed. So, we perform log transformation so that the skewness is nearly zero.

[CODE CELL]
# Determining the Skewness of data
print ("Skew is:", train.SalePrice.skew())

plt.hist(train.SalePrice)
plt.show()

# After log transformation of the data it looks much more center aligned
train['Skewed_SP'] = np.log(train['SalePrice']+1)
print ("Skew is:", train['Skewed_SP'].skew())
plt.hist(train['Skewed_SP'], color='blue')
plt.show()

[CODE CELL]
sns.catplot(x='MSSubClass', y='Skewed_SP', data=train, kind='bar', height=3, aspect=3)

fig, axis1 = plt.subplots(1, 1, figsize=(10, 3))
sns.countplot(x='MSSubClass', data=train, ax=axis1)

print(train['MSSubClass'].value_counts())

[MARKDOWN CELL]
MSSubClass = 60 has highest SalePrice while the sales of houses with MSSubClass = 20 is the highest.

[CODE CELL]
import seaborn as sns
import matplotlib.pyplot as plt

# Use catplot instead of factorplot
sns.catplot(x='MSZoning', y='Skewed_SP', data=train, kind='bar', height=3, aspect=3)

# Countplot
fig, axis1 = plt.subplots(1, 1, figsize=(10, 3))
sns.countplot(x='MSZoning', data=train, ax=axis1)

# Print value counts
print(train['MSZoning'].value_counts())

[CODE CELL]
import seaborn as sns
import matplotlib.pyplot as plt

sns.catplot(
    x='MSZoning',
    y='SalePrice',
    col='MSSubClass',
    data=train,
    kind='bar',
    col_wrap=4,
    aspect=0.8,
    palette='Set2'
)

[MARKDOWN CELL]
Lets analyze the numeric features using the numpy library

[CODE CELL]
numerical_features = train.select_dtypes(include=[np.number])
numerical_features.dtypes

[CODE CELL]
# Then we will try to find the corretation between the feature and target
corr = numerical_features.corr()
#print (corr['SalePrice'].sort_values(ascending=False)[:5], '\n')
#print (corr['SalePrice'].sort_values(ascending=False)[-5:])
print (corr['SalePrice'].sort_values(ascending=False)[:], '\n')


[MARKDOWN CELL]
We will analyze the features in their descending of correlation with sales price

[CODE CELL]
train.OverallQual.unique()

[CODE CELL]
#Creating a pivot table
quality_pivot = train.pivot_table(index='OverallQual',values='SalePrice', aggfunc=np.median)

[CODE CELL]
quality_pivot

[CODE CELL]
quality_pivot.plot(kind='bar',color='blue')
plt.xlabel('Overall Quality')
plt.ylabel('Median')
plt.xticks(rotation=0)
plt.show()

[MARKDOWN CELL]
SalePrice varies directly with the Overall quality

[CODE CELL]
sns.regplot(x='GrLivArea',y='Skewed_SP',data=train)

[MARKDOWN CELL]
SalePrice increases as the GrLivArea increases.  We will also get rid of the outliers which severely affect the prediction of the survival rate.

[CODE CELL]
#Removing outliers
train = train[train['GrLivArea'] < 4000]
sns.regplot(x='GrLivArea',y='Skewed_SP',data=train)

[CODE CELL]
sns.regplot(x='GarageArea',y='Skewed_SP',data=train)

[MARKDOWN CELL]
GarageArea and SalePrice are directly proportional.

We will again get rid of the outliers.

[CODE CELL]
#Removing outliers
train = train[train['GarageArea'] < 1200]
sns.regplot(x='GarageArea',y='Skewed_SP',data=train)

[CODE CELL]
#Removing the null values
nulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])
nulls.columns = ['Null Count']
nulls.index.name = 'Feature'
nulls

[CODE CELL]
# Pool null value refers to no pool area
print ("Unique values are:", train.MiscFeature.unique())

[CODE CELL]
#Analysing the non numeric data
categoricals = train.select_dtypes(exclude=[np.number])
categoricals.describe(include='all')

[CODE CELL]
train['Neighborhood'].value_counts().plot(kind='bar')

[CODE CELL]
import seaborn as sns
import matplotlib.pyplot as plt

# Use catplot instead of factorplot
g = sns.catplot(x='Neighborhood', y='Skewed_SP', data=train, kind='bar', aspect=3,palette='Set2')

# Rotate x-axis labels
g.set_xticklabels(rotation=90)


[CODE CELL]
train['Condition1'].value_counts()

[CODE CELL]
train['Condition2'].value_counts()

[CODE CELL]
import seaborn as sns
import matplotlib.pyplot as plt

# Use catplot instead of factorplot
g = sns.catplot(
    x='Condition1',
    y='Skewed_SP',
    col='Condition2',
    data=train,
    kind='bar',
    col_wrap=4,
    aspect=0.8,
    palette='Set2'
)

# Rotate x-axis labels
for ax in g.axes.flat:
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)


[CODE CELL]
train['SaleCondition'].value_counts()

[CODE CELL]
train['SaleType'].value_counts()

[CODE CELL]
import seaborn as sns
import matplotlib.pyplot as plt

# Use catplot instead of factorplot
g = sns.catplot(
    x='SaleCondition',
    y='Skewed_SP',
    col='SaleType',
    data=train,
    kind='bar',
    col_wrap=4,
    aspect=0.8,
    palette='Set2'
)

# Rotate x-axis labels correctly
for ax in g.axes.flat:
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

plt.show()  # Show the plot

[CODE CELL]
#Data Trasformation
print ("Original: \n")
print (train.Street.value_counts(), "\n")

[CODE CELL]
# Turn into one hot encoding
train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)
test['enc_street'] = pd.get_dummies(train.Street, drop_first=True)

[CODE CELL]
# Encoded
print ('Encoded: \n')
print (train.enc_street.value_counts())

[CODE CELL]
# Feature Engineering
condition_pivot = train.pivot_table(index='SaleCondition',
                                    values='SalePrice', aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

[CODE CELL]
def encode(x): return 1 if x == 'Partial' else 0
train['enc_condition'] = train.SaleCondition.apply(encode)
test['enc_condition'] = test.SaleCondition.apply(encode)

[CODE CELL]
condition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Encoded Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

[CODE CELL]
#Interpolation of data
data = train.select_dtypes(include=[np.number]).interpolate().dropna()

[CODE CELL]
sum(data.isnull().sum() != 0)

[CODE CELL]
# Linear Model for the  train and test
y = np.log(train.SalePrice)
X = data.drop(['SalePrice', 'Id'], axis=1)

[CODE CELL]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
                                    X, y, random_state=42, test_size=.33)

[CODE CELL]
from sklearn import linear_model
from sklearn import ensemble

#lr =  ensemble.RandomForestRegressor(n_estimators = 100, oob_score = True, n_jobs = -1,random_state =50,max_features = "sqrt", min_samples_leaf = 50)
#lr = linear_model.LinearRegression()
lr = ensemble.GradientBoostingRegressor()
#lr = linear_model.TheilSenRegressor()
#lr = linear_model.RANSACRegressor(random_state=50)

[CODE CELL]
model = lr.fit(X_train, y_train)

[CODE CELL]
print ("R^2 is: \n", model.score(X_test, y_test))

[CODE CELL]
predictions = model.predict(X_test)

[CODE CELL]
from sklearn.metrics import mean_squared_error
print ('RMSE is: \n', mean_squared_error(y_test, predictions))

[CODE CELL]
actual_values = y_test
plt.scatter(predictions, actual_values, alpha=.75,
            color='b') #alpha helps to show overlapping data
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Linear Regression Model')
#pltrandom_state=None.show()

[CODE CELL]
for i in range (-2, 3):
    alpha = 10**i
    rm = linear_model.Ridge(alpha=alpha)
    ridge_model = rm.fit(X_train, y_train)
    preds_ridge = ridge_model.predict(X_test)

    plt.scatter(preds_ridge, actual_values, alpha=.75, color='b')
    plt.xlabel('Predicted Price')
    plt.ylabel('Actual Price')
    plt.title('Ridge Regularization with alpha = {}'.format(alpha))
    overlay = 'R^2 is: {}\nRMSE is: {}'.format(
                    ridge_model.score(X_test, y_test),
                    mean_squared_error(y_test, preds_ridge))
    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')

    plt.show()

[CODE CELL]
submission = pd.DataFrame()
submission['Id'] = test.Id

[CODE CELL]
missing_features = set(X_train.columns) - set(test.columns)
print(f"Missing Features: {missing_features}")


[CODE CELL]
test['Skewed_SP'] = 0  # or another appropriate default value


[CODE CELL]
feats = test[X_train.columns]  # Select only training features
feats = feats.interpolate()  # Handle missing values if necessary
predictions = model.predict(feats)


[CODE CELL]
feats = test.select_dtypes(
        include=[np.number]).drop(['Id'], axis=1).interpolate()


[CODE CELL]
feats

[CODE CELL]
final_predictions = np.exp(predictions)

[CODE CELL]
print ("Original predictions are: \n", predictions[:5], "\n")
print ("Final predictions are: \n", final_predictions[:5])

[CODE CELL]
submission['SalePrice'] = final_predictions
submission.head()

[CODE CELL]
submission.to_csv('submission1.csv', index=False)
